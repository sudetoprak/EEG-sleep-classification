# -*- coding: utf-8 -*-
"""sdra.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fhXdFYQR2Z6luaDCrSCNgjS4S77SijUl
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd

file_path = '/content/drive/My Drive/acquiredDataset.csv'
data=pd.read_csv(file_path)

## Data handling
import numpy as np
import pandas as pd

## Data visualization
import seaborn as sns
import matplotlib.pyplot as plt

## Warnings
import warnings
warnings.filterwarnings('ignore')

print("Shape of the dataset:",data.shape)
print("Number of rows (data points):", len(data))
print("Number of columns:", data.shape[1])
print("Number of data pieces:", data.shape[0] * data.shape[1])
print(f'Data size: {data.shape[0]} rows and {data.shape[1]} columns')




missing_values = data.isna().sum()
print(missing_values)


print(data['classification'].value_counts())

variables = ['attention', 'meditation', 'delta', 'theta', 'lowAlpha', 'highAlpha',
             'lowBeta', 'highBeta', 'lowGamma', 'highGamma']

for var in variables:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='classification', y=var, data=data, palette='viridis')
    plt.title(f'{var} Distribution by Classification')
    plt.show()

X=data.drop(columns=['classification'])
Y=data['classification']

from sklearn.feature_selection import mutual_info_classif

importance = mutual_info_classif(X, Y, random_state=42)

feat_importance = pd.Series(importance, index=X.columns).sort_values(ascending=True)

plt.figure(figsize=(10, 6))
feat_importance.plot(kind="barh", color="teal")
plt.title("Feature Importance based on Mutual Information", fontsize=14)
plt.xlabel("Mutual Information Score", fontsize=12)
plt.ylabel("Features", fontsize=12)
plt.grid(axis="x", linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()

top_features = feat_importance.nlargest(9)  # En önemli 9 özelliği seç
print("Top Features:\n", top_features)

from numpy import mean
from numpy import std
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline

rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)
model = DecisionTreeClassifier()
pipeline = Pipeline(steps=[('s',rfe),('m',model)])

pipeline.fit(X, Y)

rfe = pipeline.named_steps['s']

selected_features = X.columns[rfe.support_]
print("Selected Features from Pipeline:", selected_features)

from sklearn.ensemble import RandomForestClassifier

columns_to_process = ['attention', 'meditation', 'delta', 'theta',
                      'lowAlpha', 'highAlpha', 'lowBeta', 'highBeta',
                      'lowGamma', 'highGamma']

window_size = 5

for col in columns_to_process:
    data[f'{col}_mean'] = data[col].rolling(window=window_size).mean()
    data[f'{col}_std'] = data[col].rolling(window=window_size).std()

data = data.dropna().reset_index(drop=True)

X_new = data[[f'{col}_mean' for col in columns_to_process] +
             [f'{col}_std' for col in columns_to_process]]
Y= data['classification']

model = RandomForestClassifier()
model.fit(X_new, Y)

importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

for i in range(len(indices)):
    print(f"Feature: {X_new.columns[indices[i]]}, Importance: {importances[indices[i]]}")

# Model ve veriyi hazırlama
X = data[[f'{col}_mean' for col in columns_to_process] +
             [f'{col}_std' for col in columns_to_process]]
Y = data['classification']

print("X shape:", X.shape)
print("y shape:", Y.shape)

def create_windows(X, Y, window_size, step_size):
    X_windows = []
    Y_windows = []

    for i in range(0, len(X) - window_size + 1, step_size):
        X_windows.append(X[i:i + window_size])
        Y_windows.append(Y[i + window_size - 1])

    return np.array(X_windows), np.array(Y_windows)

import pandas as pd
import numpy as np

class Pipeline:
    def __init__(self, scalar, resampler, scaler_name):
        self.scalar = scalar
        self.resampler = resampler
        self.scaler_name = scaler_name

    def fit_resample(self, X, Y):
        n_dim_flag = False
        original_shape = X.shape

        if X.ndim > 2:
            X = X.reshape(original_shape[0], -1)
            n_dim_flag = True

        X = self.scalar.fit_transform(X)

        print(f"----- Orijinal ve {self.scaler_name} Sonrası Veri Bilgileri -----")
        print(f"Orijinal eğitim veri seti boyutu: {original_shape[0]}")

        if isinstance(Y, np.ndarray):
            Y = pd.Series(Y)

        print("----- Orijinal Sınıf Dağılımı -----")
        print(Y.value_counts())
        print("----------------------------------------------------\n")

        X, Y = self.resampler.fit_resample(X, Y)
        print(f"{self.scaler_name} sonrası eğitim veri seti boyutu: {X.shape[0]}")
        print("----------------------------------------------------\n")
        print(f"----- {self.scaler_name} Sonrası Sınıf Dağılımı -----")
        print(pd.Series(Y).value_counts())
        print("----------------------------------------------------")

        if n_dim_flag:
            features_per_timestep = original_shape[2]
            timesteps = original_shape[1]
            X = X.reshape(X.shape[0], timesteps, features_per_timestep)

        return X, Y

    def transform(self, X, Y=None):
        n_dim_flag = False
        original_shape = X.shape

        # Eğer X üç boyutluysa iki boyuta indir
        if X.ndim > 2:
            X = X.reshape(original_shape[0], -1)  # Düzeltme: Şekil değiştirme mantığı
            n_dim_flag = True

        # Ölçeklendirme
        X = self.scalar.transform(X)

        if n_dim_flag:
            features_per_timestep = original_shape[2]
            timesteps = original_shape[1]
            X = X.reshape(X.shape[0], timesteps, features_per_timestep)

        return X, Y

from sklearn.preprocessing import RobustScaler
from imblearn.over_sampling import SMOTE

robust = RobustScaler()
smote = SMOTE(random_state=42)
pre_pipeline = Pipeline(robust, smote,"SMOTE")

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y,  shuffle =True)

print("Train and Test ")
print(f"X_train: {X_train.shape}, Y_train: {Y_train.shape}")
print(f"X_test: {X_test.shape}, Y_test: {Y_test.shape}")

X_train, Y_train = pre_pipeline.fit_resample(X_train, Y_train)

X_test, Y_test = pre_pipeline.transform(X_test, Y_test)

print("Sample Scaled Data")
print("X_train_scaled (first 5 lines):\n", X_train[:5])
print("\nX_test_scaled (first 5 lines):\n", X_test[:5])

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import  BatchNormalization, Dropout
from tensorflow.keras.regularizers import l1_l2


# Veriyi yeniden şekillendirme
n_features = X_train.shape[1]
X_train_reshaped = X_train.reshape((X_train.shape[0], 1, n_features))
X_test_reshaped = X_test.reshape((X_test.shape[0], 1, n_features))

model = Sequential()


model.add(LSTM(64, activation='tanh',
                 input_shape=(1, n_features),  # (timesteps, features)
               return_sequences=True))
model.add(BatchNormalization())
model.add(Dropout(0.2))

model.add(LSTM(64, activation='tanh',
               return_sequences=False))
model.add(BatchNormalization())
model.add(Dropout(0.2))

model.add(Dense(64, activation='relu',
                kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4)))
model.add(BatchNormalization())
model.add(Dropout(0.2))

model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.summary()

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard, CSVLogger
import os

def create_callbacks():
    callbacks = [
        EarlyStopping(
            monitor='val_accuracy',
            patience=30,
            restore_best_weights=True,
            verbose=1
        ),

        ModelCheckpoint(
            filepath=os.path.join('/content/', 'best_model.keras'),
            monitor='val_accuracy',
            save_best_only=True,
            mode='max',
            verbose=1
        ),

        ReduceLROnPlateau(
            monitor='val_accuracy',
            factor=0.2,
            patience=5,
            min_lr=1e-6,
            verbose=1
        ),

        TensorBoard(
            log_dir='/content/logs',
            histogram_freq=1,
            write_graph=True,
            write_images=True,
            update_freq='epoch'
        ),

        CSVLogger(
            os.path.join('/content/', 'training_log.csv'),
            separator=',',
            append=False
        )
    ]

    return callbacks

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard, CSVLogger


callbacks = create_callbacks()

# Veriyi LSTM için uygun formata dönüştürme (yeniden şekillendirme)
X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Model derleme
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Model eğitimi - yeniden şekillendirilmiş veri ile
history = model.fit(X_train_reshaped, Y_train,
                    epochs=50,
                    batch_size=64,
                    validation_data=(X_test_reshaped, Y_test),
                    callbacks=callbacks)

plt.plot(history.history['accuracy'],label='Train Accuracy')
plt.plot(history.history['val_accuracy'],label='Validation Accuracy')
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'],label='Train loss')
plt.plot(history.history['val_loss'],label='Validation loss')
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Test verisini yeniden şekillendirme
X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Tahminleri yapma
Y_pred = model.predict(X_test_reshaped)
Y_pred_classes = (Y_pred > 0.5).astype(int)

# Sınıflandırma metrikleri
from sklearn.metrics import classification_report, confusion_matrix
print("\nSınıflandırma Raporu:")
print(classification_report(Y_test, Y_pred_classes))

# Confusion Matrix görselleştirme
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 6))
cm = confusion_matrix(Y_test, Y_pred_classes)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

cm = confusion_matrix(Y_test, Y_pred_classes)
tn, fp, fn, tp = cm.ravel()

accuracy = accuracy_score(Y_test, Y_pred_classes)
precision = precision_score(Y_test, Y_pred_classes)
recall = recall_score(Y_test, Y_pred_classes)
f1 = f1_score(Y_test, Y_pred_classes)
specificity = tn / (tn + fp)

print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall (Sensitivity): {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Specificity: {specificity:.4f}")

import numpy as np
from sklearn.preprocessing import RobustScaler

# Create new data with all 20 features (10 original features + their derived features)
new_data = np.array([[
    # Original features
     # Original features
    30,      # attention (very low)
    25,      # meditation (very low)
    98700, # delta (very high)
    9980,  # theta (very high)
    5087,   # lowAlpha (high)
    5508,   # highAlpha (high)
    8090,    # lowBeta (very low)
    5087,    # highBeta (very low)
    3245,    # lowGamma (very low)
    1987,    # highGamma (very low)
    # Derived features (means)
    28,      # attention_mean
    23,      # meditation_mean
    1187, # delta_mean
    880090,  # theta_mean
    48790,   # lowAlpha_mean
    53000,   # highAlpha_mean
    7500,    # lowBeta_mean
    4800,    # highBeta_mean
    2800,    # lowGamma_mean
    900      # highGamma_mean
]])

# Initialize and fit the RobustScaler
scaler = RobustScaler()
new_data_scaled = scaler.fit_transform(new_data)

# Reshape for LSTM (samples, timesteps, features)
new_data_reshaped = new_data_scaled.reshape((1, 1, 20))

# Make prediction
prediction = model.predict(new_data_reshaped)

# Interpret results
threshold = 0.5
if prediction[0][0] < threshold:
    print("Kişi: Uykuya dalıyo")
else:
    print("Kişi: Uyanık durumda")



print(f"\nTahmin değeri: {prediction[0][0]:.4f}")